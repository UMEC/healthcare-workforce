{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical Model Pandas Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module extracts all the csv files found in a specified directory and loads them into a dictionary of pandas dataframes\n",
    "\n",
    "Each dataframe is truncated at the *end* position as specified in the first column of each imported CSV file.\n",
    "\n",
    "The provided functions allow the list of dataframes to be retrieved and access to the dataframes themselves\n",
    "\n",
    "Designed to be maintained in Jupyter Notebook to assist learning, it is converted to standard Python via : \n",
    "\n",
    "### jupyter nbconvert --to script workforce_pandas.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines if the script is running server side or in a Jupyter notebook (where the __file__ variable is not accessible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    ''' Returns jupyter if running in a notebook, otherwise returns server\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "    except:\n",
    "        return 'server'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of directory path is determined by the run-time environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_of_script()=='jupyter':\n",
    "    directory = \"../data/data_input_component_csv/\"\n",
    "else:\n",
    "    directory = os.path.abspath(os.path.dirname(os.path.realpath(__file__)) + \"/../data/data_input_component_csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes dictionary is used to create a indexed home for all the pandas data frames.  Their CSV file names, which were automatically generated from their sheet names by the Excel import service will then used as their panda names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheets array will be used to store a full list of the names of all the panda dataframes created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section has two main functions.  First, it will scan the prescribed directory for all the files with .csv as their suffix.  For each file found, a loop begins.  \n",
    "\n",
    "The name of the csv file (minus the suffix) is asigned to the sheet variable.  This is appended to the sheets array.  \n",
    "\n",
    "The csv file is then read into a pandas dataframe.  This dataframe is stored in the frames dictionary under the name of the sheet.  The dataframe is then scanned looking for the value <<end>> in the first column.  This is deemed to be the end of the dataframe and the dataframe is truncated to the row above this.\n",
    "    \n",
    "The loop continues until all the .csv files have been read and processed.\n",
    "\n",
    "Trimming the length first (according to an easy to use human clip point) then makes trimming the columns much more reliable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "sheets = []\n",
    "for f in os.listdir(directory):\n",
    "    if f.endswith(\".csv\"):l\n",
    "        \n",
    "        sheet = os.path.splitext(f)[0]\n",
    "        sheets.append(sheet)\n",
    "        dataframes[sheet] = pd.read_csv(directory + \"/\" + f)\n",
    "        for j in range(len(dataframes[sheet])):\n",
    "            if dataframes[sheet].iloc[j,0] ==\"<<end>>\":\n",
    "                break\n",
    "        dataframes[sheet] = dataframes[sheet].head(j)\n",
    "        dataframes[sheet]=dataframes[sheet].dropna(axis=1,how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_list():\n",
    "    '''\n",
    "    Returns an array containing the list of all the panda dataframes that\n",
    "    have been loaded\n",
    "    '''\n",
    "    return sheets\n",
    "def get_dataframe_dict():\n",
    "    '''\n",
    "    Returns the dictionary of all the pandas dataframes\n",
    "    '''\n",
    "    return dataframes\n",
    "def get_dataframe(name):\n",
    "    '''\n",
    "    Returns a dataframe based on the name provided, if no dataframe of \n",
    "    that name is present then a string will be returned\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            name : str, mandatory\n",
    "                The name of the dataframe requested\n",
    "    '''\n",
    "    try:\n",
    "        return dataframes[name]\n",
    "    except:\n",
    "        return \"No dataframe of that name\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
