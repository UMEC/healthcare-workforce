{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workforce Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program creates a pandas dictionary from the imported Excel spreadsheet.\n",
    "\n",
    "It can return three types of information as a result of requests from the front end:\n",
    "\n",
    "#### Sample UI data retrival commands\n",
    "\n",
    "* {\"request_type\": \"available_years\"}\n",
    "\n",
    "* {\"request_type\": \"geo_profile\"}\n",
    "\n",
    "* {\"request_type\": \"provider_profile\"}\n",
    "\n",
    "It can also persist and retrieve pickled models from the filesystem:\n",
    "\n",
    "#### Sample model save/load commands (currently not used)\n",
    "\n",
    "* {\"request_type\": \"save_model\", \"filename\":\"test_pickle\"}\n",
    "\n",
    "* {\"request_type\": \"load_model\", \"filename\": \"test_pickle\"}\n",
    "\n",
    "#### Sample model run commands\n",
    "This program will default to running ideal staffing for the State of Utah in 2018 if no other instruction is given.\n",
    "\n",
    "* {\"request_type\":\"run_model\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"State of Utah\", \"year\":\"2018\", \"option\":\"ideal_staffing\", \"sub_option\":\"all_combination\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"Garfield County\", \"year\":\"2018\", \"option\":\"ideal_staffing\", \"sub_option\":\"all_combination\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"State of Utah\", \"year\":\"2018\", \"option\":\"ideal_staffing\", \"sub_option\":\"wage_max\", \"wage_max\":\"20000\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"Wayne County\", \"year\":\"2018\", \"option\":\"ideal_staffing\", \"sub_option\":\"wage_weight\",\"wage_weight\":\"0.5\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"Beaver County\", \"year\":\"2019\", \"option\":\"ideal_staffing_current\", \"sub_option\":\"all_combination\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"Washington County\", \"year\":\"2018\", \"option\":\"ideal_staffing_current\", \"sub_option\":\"wage_max\", \"wage_max\":\"400000000\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"Rich County\", \"year\":\"2018\", \"option\":\"ideal_staffing_current\",\"sub_option\":\"wage_weight\", \"wage_weight\":\"0.75\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"Utah County\", \"year\":\"2020\", \"option\":\"service_allocation\"}\n",
    "\n",
    "* {\"request_type\":\"run_model\", \"geo\":\"State of Utah\", \"year\":\"2018\", \"option\":\"service_allocation\"}\n",
    "\n",
    "It can also process deltas to this information as a result of user input from the front end:\n",
    "\n",
    "* {JSON_string}\n",
    "\n",
    "The JSON_string will be formatted similarly to the request types.\n",
    "\n",
    "In response to these deltas, it will run the relevant optimization model depending upon the\n",
    "deltas and constraints provided in the JSON string.\n",
    "\n",
    "### jupyter nbconvert --to script workforce_model.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses the imported Excel sheets that are created by the workforce_pandas module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import workforce_pandas as wfpd\n",
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from allo_cal import main\n",
    "command=\"null\"\n",
    "provider_type=\"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_script():\n",
    "    ''' Returns jupyter if running in a notebook, otherwise returns server\n",
    "    '''\n",
    "    try:\n",
    "        ipy_str = str(type(get_ipython()))\n",
    "        if 'zmqshell' in ipy_str:\n",
    "            return 'jupyter'\n",
    "    except:\n",
    "        return 'server'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determines and stores the path for the pickle directory used to persist and retrieve models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/pickle/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if type_of_script()=='jupyter':\n",
    "    directory = r\"../data/pickle/\"\n",
    "else:\n",
    "    directory = os.path.abspath(os.path.dirname(os.path.realpath(__file__)) + r\"/../data/pickle/\")\n",
    "directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default values for model state, should they not be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = \"State of Utah\"\n",
    "year =\"2018\"\n",
    "option = \"ideal_staffing\"\n",
    "sub_option = \"all_combination\"\n",
    "wage_max = \"null\"\n",
    "wage_weight = \"null\"\n",
    "collapse_group = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pop_chronic_trend',\n",
       " 'pop_chronic_prev',\n",
       " 'chron_care_freq',\n",
       " 'geo_area_list',\n",
       " 'service_characteristics',\n",
       " 'pop_acute_need',\n",
       " 'frontsheet',\n",
       " 'population',\n",
       " 'provider_supply',\n",
       " 'pop_prev_need',\n",
       " 'provider_list',\n",
       " 'encounter_detail',\n",
       " 'metadata',\n",
       " 'overhead_work',\n",
       " 'encounter_types',\n",
       " 'index']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfpd.sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes the input of a JSON string from stdin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"request_type\":\"run_model\", \"geo\":\"State of Utah\", \"year\":\"2018\", \"option\":\"ideal_staffing\", \"sub_option\":\"all_combination\"}\n"
     ]
    }
   ],
   "source": [
    "value = \"null\"\n",
    "command_string = input(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All responses are formatted and sent using the respond function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(response_msg,verb,object,error_msg=None):\n",
    "    \"\"\"Prints a JSON formatted service response to std out and exits the program\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        response_msg : str, mandatory\n",
    "            The message included in the response attribute of the JSON out\n",
    "        verb : str, mandatory\n",
    "            The command that was received (if any)\n",
    "        object: str, mandatory\n",
    "            The target for the command that was received (if any)\n",
    "        error_msg : str, optional\n",
    "            An error message string\n",
    "    \"\"\"\n",
    "    if (error_msg != None):\n",
    "        response_msg = '{\"error_msg\":\"' + error_msg + '\"}'\n",
    "    response = '{\"request\":' + command_string + ',\"response\":' + response_msg + '}'\n",
    "    print (response)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This input command is parsed into one to three strings, or an exception is raised and then passed back to the caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the command line argument into a JSON object\n",
    "try:\n",
    "    parsed_command = json.loads(command_string)\n",
    "except Exception as e:\n",
    "    respond(None,command,provider_type, \"ERROR: Invalid argument - not in JSON format: \"+str(e))\n",
    "if \"request_type\" in parsed_command:\n",
    "    command = str(parsed_command[\"request_type\"])\n",
    "else:\n",
    "    respond(None,command,provider_type, \"ERROR: Invalid argument - no request_type defined\")\n",
    "if \"filename\" in parsed_command:\n",
    "    filename = str(parsed_command[\"filename\"])\n",
    "if \"geo\" in parsed_command:\n",
    "    geo = str(parsed_command[\"geo\"])\n",
    "if \"year\" in parsed_command:\n",
    "    year = str(parsed_command[\"year\"])\n",
    "if \"option\" in parsed_command:\n",
    "    option = str(parsed_command[\"option\"])    \n",
    "if \"sub_option\" in parsed_command:\n",
    "    sub_option = str(parsed_command[\"sub_option\"])\n",
    "if \"wage_max\" in parsed_command:\n",
    "    wage_max = str(parsed_command[\"wage_max\"])\n",
    "if \"wage_weight\" in parsed_command:\n",
    "    wage_weight = str(parsed_command[\"wage_weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions are used to manipulate the automatically generated JSON strings as they often don't conform to the format that we require in our responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_brackets(JSON_string):\n",
    "    \"\"\"Strips the square brackets from a JSON string\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        JSON_string : str, mandatory\n",
    "            The JSON string to strip the leading and trailing end square brackets from\n",
    "    \"\"\"\n",
    "    result = JSON_string.strip(\"[]\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_curlies(JSON_string):\n",
    "    \"\"\"Strips the curly brackets from a JSON string\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        JSON_string : str, mandatory\n",
    "            The JSON string to strip the leading and trailing curly brackets from\n",
    "    \"\"\"\n",
    "    result = JSON_string.strip(\"{}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions manipulate pandas dataframes in various ways to assist in turning them into JSON strings.  _df_to_json_attri is useful for single simple rows and uses the in-built functions to perform the transform.  The _sub_json_object_ and _frame_sub_json_object_ manipulate the dataframes themselves to categories of related data in dataframes for conversion into JSON strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_json_object(source,index_column,value):\n",
    "    \"\"\"Takes a wfpd dataframe (i.e. a csv sheet in pandas form)\n",
    "        and filters it on provided value on its index_column.  The\n",
    "        index column is then removed, effectively providing a table\n",
    "        of related data.  This table is then returned as a dataframe.\n",
    "        Useful function for transforming pandas JSON friendly structures.\n",
    "    \n",
    "        Works well for simple rows, but is a little naive in terms of transformation for\n",
    "        multiple row tables or those with categories.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : string\n",
    "            The name of the wfpd dataframe (aka Excel sheet name)\n",
    "        index_column : string\n",
    "            The name of the column on the wfpd dataframe to filter on\n",
    "        value : string\n",
    "            The value to filter on\n",
    "    \"\"\"\n",
    "    dataframe = wfpd.dataframes[source]\n",
    "    dataframe = dataframe.loc[dataframe[index_column] == value]\n",
    "    dataframe = dataframe.drop(index_column,1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_sub_json_object(dataframe,index_column,value):\n",
    "    \"\"\" Essentially the same as the sub_json_object function, but can take an\n",
    "        arbitrary dataframe, rather than a wfpd dataframe.\n",
    "        \n",
    "        Takes a dataframe and filters it on a provided value on its index_column.\n",
    "        The index column is then removed, effectively providing a table\n",
    "        of related data.  This table is then returned as a dataframe.\n",
    "        Useful function for transforming pandas JSON friendly structures.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        source : string\n",
    "            The name of the wfpd dataframe (aka Excel sheet name)\n",
    "        index_column : string\n",
    "            The name of the column on the wfpd dataframe to filter on\n",
    "        value : string\n",
    "            The value to filter on\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.loc[dataframe[index_column] == value]\n",
    "    dataframe = dataframe.drop(index_column,1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions manipulate dataframes directly into JSON.  The _df_to_json_attribs_ takes a dataframe with a primary key_column and iterates through the values of that column and turns each row into a JSON object.  The _df_to_json_ function simply uses the standard pandas to JSON function to convert a single row into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json_attribs(dataframe,key_column):\n",
    "    \"\"\" Iterates through each row in a dataframe that has a primary unique key\n",
    "        and turns it into a JSON string\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe : pd\n",
    "            The name of the wfpd dataframe (aka Excel sheet name)\n",
    "        key_column : string\n",
    "            The name of the column on the wfpd dataframe to filter on\n",
    "    \"\"\"    \n",
    "    elements = len(dataframe)\n",
    "    json = \"{\"\n",
    "    count = 0\n",
    "    for index, row in dataframe.iterrows():\n",
    "        element = row[key_column]\n",
    "        json = json + '\"' + element + '\":{'\n",
    "        attribute_frame = dataframe.loc[dataframe[key_column] == element]\n",
    "        attribute_frame = attribute_frame.drop(key_column,1)\n",
    "        json = json + strip_curlies(strip_brackets(df_to_json(attribute_frame)))\n",
    "        if count != (elements-1):\n",
    "            json = json + \"},\"\n",
    "        else:\n",
    "            json = json + \"}\"\n",
    "        count = count + 1\n",
    "    json = json + \"}\"\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(dataframe):\n",
    "    \"\"\"Turns a pandas dataframe into a JSON string.\n",
    "    \n",
    "        Works file for single rows, but is a little naive in terms of transformation for\n",
    "        multiple row tables\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe : pd, mandatory\n",
    "            The pandas dataframe to turn into a JSON string\n",
    "    \"\"\"\n",
    "    json = dataframe.to_json(orient='records')\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the available years in the model; the user can select the individual year they wish to look at and send this back to the model.  At the moment, the routine deliberately restricts the return to the years 2018 to 2027.  Future enhancements may want to increase the amount of population data sent to the front end and provide a sliding window based on the current date..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_years():\n",
    "    \"\"\"Returns a JSON string of the available years in the model.\n",
    "    \"\"\"\n",
    "    out = \"\"\n",
    "    my_dataframe = wfpd.dataframes['population']\n",
    "    list=my_dataframe.columns.values[3:13]\n",
    "    out = ('{ \"available_years\":[')\n",
    "    years = len(list)\n",
    "    year_count=0\n",
    "    for item in list:\n",
    "        out = out + (json.dumps(item))\n",
    "        if year_count != (years -1):\n",
    "            out = out + (\",\")\n",
    "        else:\n",
    "            out = out + (\"]}\")\n",
    "        year_count = year_count + 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the data relevant to each geographic area, by geographic area.  This includes the sdoh index and the details of the primary care providers in each of the geographic areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_profile():\n",
    "    \"\"\" Creates a easily addressable JSON string from the wfpd pandas dataframes that\n",
    "        reports back to the front end the following data:\n",
    "        \n",
    "        By county/area:\n",
    "            area sdoh index\n",
    "            area provider supply by provider_type, for each provider/area combination\n",
    "                number of providers\n",
    "                growth trend\n",
    "                mean_wage\n",
    "    \"\"\" \n",
    "    primary_key_df_name = 'geo_area_list'\n",
    "    primary_key_column = 'geo_area'\n",
    "    primary_key_dataframe = wfpd.dataframes[primary_key_df_name]\n",
    "    elements = len(primary_key_dataframe)\n",
    "    #print (elements)\n",
    "    out = \"{\"\n",
    "    for index, row in primary_key_dataframe.iterrows():\n",
    "        element = row[primary_key_column]\n",
    "        out = out + '\"' + element + '\":{'\n",
    "        out = out + strip_curlies(strip_brackets(df_to_json(sub_json_object(primary_key_df_name,primary_key_column,element)))) + \",\"  \n",
    "        out = out + '\"supply\":'\n",
    "        out = out + df_to_json_attribs(sub_json_object('provider_supply','provider_geo_area',element),'provider_abbr') \n",
    "        if index != elements-1:\n",
    "            out = out + \"},\"\n",
    "        else:\n",
    "            out = out + \"}\"\n",
    "    out = out + \"}\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each provider this function provides a category based view of which services they can provide - and how suitable those services are for them to carry out.  For each service a minimum and maximum face to face time is defined.  It also provides a supply based view of each provider supply by county, including the numbers per county, the expected growth rate, mean wage and wage trends.\n",
    "\n",
    "NB: As the following function demonstrates, constructing easily navigable JSON from pandas is a non-trivial operation.  In retrospect, the creation of the JSON string should have been done by programmatically building a Python structure that is capable of being serialised then serialising it.  For more information see [here](https://realpython.com/python-json/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provider_profile():\n",
    "    \"\"\" Creates a easily addressable JSON string from the wfpd pandas dataframes that\n",
    "        reports back to the front end the following data:\n",
    "        \n",
    "        By provider, indexed on their abbreviated name:\n",
    "            the full name of the provider\n",
    "            the serive categories they support\n",
    "                the services that they support\n",
    "                    the max f2f time\n",
    "                    the min f2f time\n",
    "                    where the service is on their licence to operate\n",
    "            the supply profile of each provider by county\n",
    "                number of FTEs\n",
    "                growth trend\n",
    "                mean wage\n",
    "                wage trend\n",
    "                \n",
    "    \"\"\" \n",
    "    out = \"\"\n",
    "    primary_key_df_name = 'provider_list'\n",
    "    primary_key_column = 'provider_abbr'\n",
    "    primary_key_dataframe = wfpd.dataframes[primary_key_df_name]\n",
    "    # we will iterate the outer loop once for each provider type\n",
    "    elements = len(primary_key_dataframe)\n",
    "    # open the JSON string and set loop counter to zero\n",
    "    out = \"{\"\n",
    "    count = 0\n",
    "    # create a loop that will iterate through each row in the provider list\n",
    "    for index, row in primary_key_dataframe.iterrows():\n",
    "        # extract the provider name\n",
    "        element = row[primary_key_column]\n",
    "        # open a JSON object that correspondes to the provider type\n",
    "        out = out + '\"' + element + '\":{'\n",
    "        # write to the JSON all the top level attributes of the provider\n",
    "        out = out + strip_curlies(strip_brackets(df_to_json(sub_json_object(primary_key_df_name,primary_key_column,element))))\n",
    "        # retrieve the dataframe that contains the provider/serice matrix\n",
    "        pa_services = wfpd.dataframes['service_characteristics']\n",
    "        # change N/A to 'no' to prevent processing errors\n",
    "        pa_services = pa_services.fillna(\"no\")\n",
    "        # filter the dataframe to just those things this provider can do\n",
    "        pa_services = pa_services.loc[pa_services[element] != \"no\"]\n",
    "        # now strip the dataframe back to the information we need (including the provider column)\n",
    "        pa_services = pa_services[['svc_category','svc_desc','min_f2f_time','max_f2f_time',element]]\n",
    "        pa_services = pa_services.rename(columns={element:'score'})    \n",
    "        #  for each of the services identified, create a unique list of the service categories they belong to\n",
    "        service_category_list = pa_services['svc_category'].unique()\n",
    "        # we can now work out how many categories we need to loop around\n",
    "        inner_element_size = len(service_category_list)\n",
    "        # now we add a second object to the provider type which contains information about the services they\n",
    "        # can perform\n",
    "        out = out + \",\"\n",
    "        out = out + '\"services:\":['\n",
    "        count3 = 0\n",
    "        # we're now going to loop for every row in the service_category array\n",
    "        for row2 in service_category_list:\n",
    "            # take a copy of pa_services as we will be filtering it\n",
    "            inner_pa_service = pa_services\n",
    "            key_column = 'svc_category'\n",
    "            inner_element = row2\n",
    "            # define the service category name as an array\n",
    "            out = out + '{\"'+inner_element+'\":['\n",
    "\n",
    "            inner_pa_service = inner_pa_service.loc[inner_pa_service[key_column] == row2]\n",
    "            inner_pa_service = inner_pa_service.drop(key_column,1)\n",
    "            #print (inner_pa_service)\n",
    "            ii_key_column = 'svc_desc'\n",
    "            ii_elements = len(inner_pa_service)\n",
    "            count2 = 0\n",
    "            for index3, row3 in inner_pa_service.iterrows():\n",
    "                ii_element = row3[ii_key_column]\n",
    "                #print (\"      Very inner element:\" + ii_element)\n",
    "                out = out + '{\"' + ii_element + '\":{'\n",
    "                out = out + strip_curlies(strip_brackets(df_to_json(frame_sub_json_object(inner_pa_service,ii_key_column,ii_element))))\n",
    "                if count2 != (ii_elements-1) :\n",
    "                    out = out + \"}},\"\n",
    "                else:\n",
    "                    out = out + \"}}\"\n",
    "                count2 = count2 + 1\n",
    "            if count3 != (inner_element_size-1) :\n",
    "                out = out + \"]},\"\n",
    "            else:\n",
    "                out = out + \"]}\"\n",
    "            count3 = count3 + 1 \n",
    "        out = out + \"],\"\n",
    "        out = out + '\"supply\":'\n",
    "        out = out + df_to_json_attribs(sub_json_object('provider_supply','provider_abbr',element),'provider_geo_area') \n",
    "        if count != elements-1:\n",
    "            out = out + \"},\"\n",
    "        else:\n",
    "            out = out + \"}\"\n",
    "        count = count + 1\n",
    "    out = out + \"}\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions will serialize the pandas dictionary into a file and then retrieve them.  \n",
    "\n",
    "NB The commented out version of the save_model is less efficient, but results in a string that could be transmitted as part of a JSON string, rather than saved directly to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename):\n",
    "    \"\"\"Turns our dictionary of pandas dataframes into a serialised form and saves it to disk\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, mandatory\n",
    "            The name of the file to be used\n",
    "    \"\"\"\n",
    "    #out = str(base64.b64encode(bytes(str(pickle.dumps(wfpd.dataframes,protocol=pickle.HIGHEST_PROTOCOL)), 'utf-8')))\n",
    "    #out = '\"' + out + '\"'\n",
    "    os.chdir(directory)\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(wfpd.dataframes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return '\"Model saved.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    \"\"\"Turns pickled files back into a dictionary of pandas dataframes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, mandatory\n",
    "            The name of the file to be loaded\n",
    "    \"\"\"\n",
    "    # global\n",
    "    # retrieve file?\n",
    "    # wfpd.dataframes = pickle.loads(string)\n",
    "    os.chdir(directory)\n",
    "    with open(filename, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return '\"Model loaded.\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical model currently supports three optimisers.  These each have their own options.  The three optimisations are:\n",
    "\n",
    "*Ideal Staffing* (greenfield model)\n",
    "This model will use Linear Programming to simply calculate the optimum number and type of individuals to meet the clinical needs of the target population (at a county or state level).  The analysis can be influenced by contraints that include the maximum budget and/or a tradeoff between suitability (i.e. a balanced LTO score) and wage cost.\n",
    "\n",
    "*Ideal Staffing Current* (brownfield model)\n",
    "This model starts with an existing provider profile and uses clinical need and LTO to maximise the efficiency of the staff profile whilst minimizing staff changes.\n",
    "\n",
    "*Service Allocation* (sharing the load)\n",
    "This model attempts to equally spread the load of services across an existing population of provider types by allocating work in such a way that the burden of care is shared as equally as it can be.  This may be useful in situation where staff numbers are difficult to change quickly.\n",
    "\n",
    "Further details on the options are in the parameters descriptions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(geo,year,option,sub_option,wage_max,wage_weight):\n",
    "    \"\"\"Runs the model optimizers based on a series of UI deltas overlaid on\n",
    "        the dictionary of pandas dataframes (which represent the Excel model input) \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        geo : str, mandatory\n",
    "            The area on which to conduct the analysis\n",
    "        year : str, mandatory\n",
    "            The year to be analysed (currently not used)\n",
    "        option : str, mandatory\n",
    "            One of - 'ideal_staffing', 'ideal_staffing_current', 'service_allocation'\n",
    "        sub_option : str, mandatory\n",
    "            One of - 'all_combination', 'wage_max', 'wage_weight'\n",
    "        wage_max : str, optional\n",
    "            A string representation of an integer, only needed if sub_option is wage_max\n",
    "        wage_weight : str, optional\n",
    "            A string representation of a float between 0 and 1, only needed if sub_option is wage_weight\n",
    "    \"\"\"  \n",
    "    # validates options and sets UI based parameters\n",
    "    sub_option_value = None\n",
    "    pos_option = ('ideal_staffing', 'ideal_staffing_current', 'service_allocation')\n",
    "    pos_sub_option = (\"all_combination\", \"wage_max\", \"wage_weight\")\n",
    "    if ( (option not in pos_option) | (sub_option not in pos_sub_option) ):\n",
    "        respond(None,command,\"null\", \"ERROR: unknown model calculation options\")\n",
    "    if sub_option == \"wage_max\":\n",
    "        sub_option_value = int(wage_max)\n",
    "    elif sub_option == \"wage_weight\":\n",
    "        sub_option_value = float(wage_weight)\n",
    "    \n",
    "    # gets the latest version of the dictionary dataframes to pass into the \n",
    "    # optimizer model.  This ensures that any deltas processed from the UI are taken\n",
    "    # into account\n",
    "    pop_chronic_trend = wfpd.dataframes['pop_chronic_trend']\n",
    "    pop_chronic_prev = wfpd.dataframes['pop_chronic_prev']\n",
    "    chron_care_freq = wfpd.dataframes['chron_care_freq']\n",
    "    geo_area = wfpd.dataframes['geo_area_list']\n",
    "    service_characteristics = wfpd.dataframes['service_characteristics']\n",
    "    pop_acute_need = wfpd.dataframes['pop_acute_need']\n",
    "    population = wfpd.dataframes['population']\n",
    "    provider_supply = wfpd.dataframes['provider_supply']\n",
    "    pop_prev_need = wfpd.dataframes['pop_prev_need']\n",
    "    provider_list = wfpd.dataframes['provider_list']\n",
    "    encounter_detail = wfpd.dataframes['encounter_detail']\n",
    "    overhead_work = wfpd.dataframes['overhead_work']\n",
    "    sdoh_score = geo_area.loc[geo_area['geo_area'] == geo,'sdoh_index']\n",
    "    \n",
    "    # additional parameters used to call the model but not currently\n",
    "    # in the spreadsheet or the user interface\n",
    "    \n",
    "    sut_target = 0.8 # sutability target 0.8 is ideal status\n",
    "    sdoh_target = 3 # Social Determinance of Health - currently impact to F2F: if a county's SDoH < target, then using minimum F2F\n",
    "    FTE_time = 60*2080 # perhaps default 124,800\n",
    "    \n",
    "    # call the model\n",
    "    out = main(geo, year, option, sub_option, sub_option_value, sut_target, sdoh_target, collapse_group, FTE_time, \n",
    "     sdoh_score, pop_chronic_trend,  pop_chronic_prev, chron_care_freq, \n",
    "             geo_area, service_characteristics, pop_acute_need , population, provider_supply , pop_prev_need , \n",
    "             provider_list , encounter_detail, overhead_work)\n",
    "    # return the results dictionary\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This continues the main processing routine of the program and farms out the different request types out to varying functions.  Each of them returns a result as a JSON string that is then returned to the caller via a respond message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"request\":{\"request_type\":\"run_model\", \"geo\":\"State of Utah\", \"year\":\"2018\", \"option\":\"ideal_staffing\", \"sub_option\":\"all_combination\"},\"response\":{\"wage_weight:0.0\": {\"total_wage\": 1171318813026.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 4964925.5, \"PA\": 48702.5, \"NP\": 1266262.5, \"RN\": 48702.5, \"PharmD\": 0.0, \"MA\": 2532524.5, \"Educ\": 0.0, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 0.0, \"MFT\": 1366389.0}}, \"wage_weight:0.1\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.2\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.3\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.4\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.5\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.6\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.7\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.8\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:0.9\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}, \"wage_weight:1.0\": {\"total_wage\": 729535849334.0, \"total_sutab\": 0.375, \"FTE\": {\"Phys\": 1461072.0, \"PA\": 48702.5, \"NP\": 1449126.5, \"RN\": 1578818.0, \"PharmD\": 0.0, \"MA\": 3365334.0, \"Educ\": 930328.5, \"Psych\": 0.0, \"LCSW\": 0.0, \"CMHC\": 1394125.0, \"MFT\": 0.0}}}}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopkira/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if command == \"available_years\":\n",
    "    result = available_years() # respond to UI request for trending/trend information\n",
    "elif command == \"geo_profile\":  \n",
    "    result = geo_profile() # respond to UI request for geographic area information\n",
    "elif command == \"provider_profile\": \n",
    "    result = provider_profile() # respond to UI request for provider information\n",
    "elif command == \"save_model\":\n",
    "    result = save_model(filename) # restore/unpic#kle the current dataframe dictionary\n",
    "elif command == \"load_model\":\n",
    "    result = load_model(filename) # pickle the current dataframe dictionary\n",
    "elif command == \"run_model\": \n",
    "    result = run_model(geo,year,option,sub_option,wage_max,wage_weight) # run the optimizer\n",
    "    #result = process_result(result) # process complex result into a JSON string\n",
    "else:\n",
    "    respond(None,command,provider_type, \"ERROR: Unknown function call.\")\n",
    "respond(str(result),str(command),str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
